# LLM Server Configuration
# ========================

# Main LLM Model Configuration
AM_MODEL_PATH=C:\models\Qwen3-4B-Instruct-2507\Qwen3-4B-Instruct-2507-Q8_0.gguf
AM_LLM_MODEL=Qwen3-4b-instruct-2507

# Embedding Model Configuration
AM_EMBEDDING_MODEL_PATH=C:\models\Qwen3-Embedding-0.6B\qwen3-embedding-0.6b-q8_0.gguf
AM_EMBEDDING_PORT=8002
AM_EMBEDDING_DIMENSIONS=1024  # Common: 384, 768, 1024, 1536
AM_EMBEDDING_POOLING=mean  # Options: mean, cls

# Hardware Configuration Profiles
# ================================
# Choose ONE profile below by uncommenting the appropriate section

# --- HIGH-END HARDWARE (RTX 4090/3090, 32GB+ RAM, 16+ cores) ---
# Optimized for maximum performance with large models
AM_CONTEXT_WINDOW=32768
AM_GPU_LAYERS=-1  # Use all layers on GPU
AM_THREADS=8  # Use half of CPU cores for best performance
AM_BATCH_SIZE=2048
AM_UBATCH_SIZE=512
AM_PARALLEL_SEQUENCES=4
AM_CONTINUOUS_BATCHING=true

# --- MID-RANGE HARDWARE (RTX 3070/3060, 16GB RAM, 8 cores) ---
# Balanced performance for medium models
# AM_CONTEXT_WINDOW=8192
# AM_GPU_LAYERS=35  # Partial offloading
# AM_THREADS=4
# AM_BATCH_SIZE=512
# AM_UBATCH_SIZE=128
# AM_PARALLEL_SEQUENCES=2
# AM_CONTINUOUS_BATCHING=true

# --- LOW-END HARDWARE (GTX 1660/CPU only, 8GB RAM, 4 cores) ---
# Conservative settings for smaller models
# AM_CONTEXT_WINDOW=4096
# AM_GPU_LAYERS=0  # CPU only
# AM_THREADS=2
# AM_BATCH_SIZE=128
# AM_UBATCH_SIZE=32
# AM_PARALLEL_SEQUENCES=1
# AM_CONTINUOUS_BATCHING=false

# --- APPLE SILICON (M1/M2/M3) ---
# Optimized for Metal Performance Shaders
# AM_CONTEXT_WINDOW=16384
# AM_GPU_LAYERS=-1  # Use Metal
# AM_THREADS=4  # Efficiency cores
# AM_BATCH_SIZE=1024
# AM_UBATCH_SIZE=256
# AM_PARALLEL_SEQUENCES=2
# AM_CONTINUOUS_BATCHING=true

# Memory System Configuration
# ===========================
AM_LLM_BASE_URL=http://localhost:8000/v1
AM_DB_PATH=./data/amemory.sqlite3
AM_INDEX_PATH=./data/faiss.index

# Advanced Performance Tuning
# ===========================
# AM_LLAMA_PORT=8000  # LLM server port
# AM_LLAMA_BATCH_SIZE=2048  # Override batch size
# AM_LLAMA_UBATCH_SIZE=512  # Override ubatch size
# AM_NO_MMAP=true  # Keep model in RAM (faster but uses more memory)
# AM_MLOCK=true  # Lock model in memory (prevents swapping)
# AM_FLASH_ATTENTION=false  # Enable flash attention (not all GPUs support)

# Retrieval Weights (must sum to 1.0)
# ====================================
AM_W_SEMANTIC=0.68
AM_W_RECENCY=0.02
AM_W_ACTOR=0.10
AM_W_SPATIAL=0.10
AM_W_USAGE=0.05
