# LLM Server Configuration
# ========================

# Main LLM Model Configuration
AM_MODEL_PATH=C:\models\Qwen3-4B-Instruct-2507\Qwen3-4B-Instruct-2507-Q8_0.gguf
AM_LLM_MODEL=Qwen3-4b-instruct-2507

# Embedding Model Configuration
AM_EMBEDDING_MODEL_PATH=C:\models\Qwen3-Embedding-0.6B\qwen3-embedding-0.6b-q8_0.gguf
AM_EMBEDDING_PORT=8002
AM_EMBEDDING_DIMENSIONS=1024  # Common: 384, 768, 1024, 1536
AM_EMBEDDING_POOLING=mean  # Options: mean, cls

# Hardware Configuration (Currently using HIGH-END profile)
# ===========================================================
# Adjust these based on your hardware:
# - HIGH-END: RTX 4090/3090, 32GB+ RAM, 16+ cores
# - MID-RANGE: RTX 3070/3060, 16GB RAM, 8 cores
# - LOW-END: GTX 1660/CPU only, 8GB RAM, 4 cores
# - APPLE: M1/M2/M3 with Metal support

# Core settings
AM_CONTEXT_WINDOW=20480  # Context size for model
AM_GPU_LAYERS=-1  # -1 = use all GPU layers
AM_THREADS=8  # Number of CPU threads
AM_BATCH_SIZE=20480  # Batch size for processing
AM_UBATCH_SIZE=1024  # Micro-batch size
AM_PARALLEL_SEQUENCES=1  # Number of parallel sequences
AM_CONTINUOUS_BATCHING=true  # Enable continuous batching

# Additional optimizations using new flexible flag system
# These are applied ON TOP of the legacy settings above
AM_LLAMA_FLAG_no_mmap=true  # Keep model in VRAM for faster access
AM_LLAMA_FLAG_mlock=true  # Lock model in memory (prevent swapping)
# AM_LLAMA_FLAG_flash_attn=true  # Uncomment if your GPU supports flash attention

# Optional: Add more performance flags as needed
# AM_LLAMA_FLAG_cache_type_k=q8_0  # Quantize KV cache for memory savings
# AM_LLAMA_FLAG_cache_type_v=q4_0  # Further quantization for V cache
# AM_LLAMA_FLAG_rope_scaling=yarn  # Use YaRN for better long context
# AM_LLAMA_FLAG_rope_freq_base=10000  # RoPE frequency base

# Memory System Configuration
# ===========================
AM_LLM_BASE_URL=http://localhost:8000/v1
AM_DB_PATH=./data/amemory.sqlite3
AM_INDEX_PATH=./data/faiss.index

# Retrieval Weights (must sum to 1.0)
# ====================================
AM_W_SEMANTIC=0.68
AM_W_RECENCY=0.02
AM_W_ACTOR=0.10
AM_W_SPATIAL=0.10
AM_W_USAGE=0.05

# ============================================================================
# ADDITIONAL LLAMA.CPP SERVER FLAGS (Using new flexible system)
# ============================================================================
# You can add ANY llama.cpp server flag without modifying code!
# Format: AM_LLAMA_FLAG_<flag_name> for LLM server
#         AM_EMBEDDING_FLAG_<flag_name> for embedding server
# Replace hyphens with underscores in flag names

# Server monitoring and debugging
# --------------------------------
# AM_LLAMA_FLAG_metrics=true  # Enable Prometheus metrics endpoint
# AM_LLAMA_FLAG_verbose=true  # Verbose logging for debugging
# AM_LLAMA_FLAG_log_colors=true  # Colored log output
# AM_LLAMA_FLAG_log_file=./logs/llama_server.log  # Log to file

# Performance optimizations
# -------------------------
# AM_LLAMA_FLAG_cache_type_k=q8_0  # Quantize K cache (saves memory)
# AM_LLAMA_FLAG_cache_type_v=q4_0  # Quantize V cache (saves more memory)
# AM_LLAMA_FLAG_cache_reuse=256  # Min chunk size for KV cache reuse
# AM_LLAMA_FLAG_threads_http=8  # HTTP request handling threads
# AM_LLAMA_FLAG_threads_batch=4  # Batch processing threads

# Context and RoPE settings
# -------------------------
# AM_LLAMA_FLAG_rope_scaling=yarn  # Better long context handling
# AM_LLAMA_FLAG_rope_freq_base=10000  # RoPE frequency base
# AM_LLAMA_FLAG_yarn_ext_factor=1.0  # YaRN extrapolation factor
# AM_LLAMA_FLAG_yarn_attn_factor=1.0  # YaRN attention scaling

# API and security
# ----------------
# AM_LLAMA_FLAG_api_key=sk-your-secure-key-here  # API authentication
# AM_LLAMA_FLAG_timeout=600  # Request timeout in seconds
# AM_LLAMA_FLAG_ssl_key_file=/path/to/key.pem  # SSL key
# AM_LLAMA_FLAG_ssl_cert_file=/path/to/cert.pem  # SSL certificate

# Chat and templating
# -------------------
# AM_LLAMA_FLAG_jinja=true  # Enable Jinja templating
# AM_LLAMA_FLAG_chat_template=llama3  # Use specific chat template
# AM_LLAMA_FLAG_no_prefill_assistant=true  # Don't prefill assistant response

# Advanced features
# -----------------
# AM_LLAMA_FLAG_model_draft=C:\models\draft.gguf  # Speculative decoding
# AM_LLAMA_FLAG_draft_max=16  # Max draft tokens
# AM_LLAMA_FLAG_lora=C:\models\adapter.gguf  # LoRA adapter
# AM_LLAMA_FLAG_mmproj=C:\models\mmproj.gguf  # Multimodal projector

# Embedding server specific
# -------------------------
# AM_EMBEDDING_FLAG_metrics=true  # Metrics for embedding server
# AM_EMBEDDING_FLAG_reranking=true  # Enable reranking endpoint
